{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPeu5OUNWnN8PLDwQXd5Zl3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b820b91c8fc44aa9998aa3929678c060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93707e54fee04ed9b867d878b3e4de06",
              "IPY_MODEL_6f9687b6a4304819a85d8a0dc27759cd",
              "IPY_MODEL_55a6c339acd84568a9125fc2c9884df3"
            ],
            "layout": "IPY_MODEL_18d20473943443efbb51c9d0d273b197"
          }
        },
        "93707e54fee04ed9b867d878b3e4de06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58d101622ee1437c9bb041dcf2dbd1f9",
            "placeholder": "​",
            "style": "IPY_MODEL_65ed59babe9b4227afd36353c82c3ae0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6f9687b6a4304819a85d8a0dc27759cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a570da9ce0340cbba76276445a507c1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e30bddaa05848e281d07d04dd87ac4e",
            "value": 2
          }
        },
        "55a6c339acd84568a9125fc2c9884df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82fd83312745477f9dcc1055941cfcf9",
            "placeholder": "​",
            "style": "IPY_MODEL_7ea79036d00446f9a66fff3d167ce99f",
            "value": " 2/2 [01:21&lt;00:00, 37.87s/it]"
          }
        },
        "18d20473943443efbb51c9d0d273b197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58d101622ee1437c9bb041dcf2dbd1f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65ed59babe9b4227afd36353c82c3ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a570da9ce0340cbba76276445a507c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e30bddaa05848e281d07d04dd87ac4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82fd83312745477f9dcc1055941cfcf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea79036d00446f9a66fff3d167ce99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnuy/AI-ML/blob/main/TrainMistralSentiAnalysisSmall4bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets\n",
        "%pip install peft transformers trl accelerate\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ap4YvScrzd8l",
        "outputId": "59cdd4c5-e0d7-47a2-e782-896127f53d2a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.47.0)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356,
          "referenced_widgets": [
            "b820b91c8fc44aa9998aa3929678c060",
            "93707e54fee04ed9b867d878b3e4de06",
            "6f9687b6a4304819a85d8a0dc27759cd",
            "55a6c339acd84568a9125fc2c9884df3",
            "18d20473943443efbb51c9d0d273b197",
            "58d101622ee1437c9bb041dcf2dbd1f9",
            "65ed59babe9b4227afd36353c82c3ae0",
            "7a570da9ce0340cbba76276445a507c1",
            "4e30bddaa05848e281d07d04dd87ac4e",
            "82fd83312745477f9dcc1055941cfcf9",
            "7ea79036d00446f9a66fff3d167ce99f"
          ]
        },
        "id": "MEqiCaR9zWOk",
        "outputId": "36fa7c65-0b82-4062-efd3-2c4300664d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: Tesla T4\n",
            "GPU memory: 15.8 GB\n",
            "Using GPU acceleration with optimized settings\n",
            "Loading mistralai/Mistral-7B-v0.1 with memory optimizations...\n",
            "Using 4-bit quantization for memory efficiency\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b820b91c8fc44aa9998aa3929678c060"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trainable parameters:\n",
            "trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n",
            "Dataset size: 15\n",
            "\n",
            "Sample data:\n",
            "                                                text     label\n",
            "0  I love this product! It's amazing and works pe...  positive\n",
            "1  This is the worst purchase I've ever made. Com...  negative\n",
            "2  The item is okay, nothing special but does the...   neutral\n",
            "3   Fantastic quality! Highly recommend to everyone.  positive\n",
            "4  Terrible customer service and poor product qua...  negative\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import os\n",
        "\n",
        "class MistralFineTuner:\n",
        "    def __init__(self, model_name=\"mistralai/Mistral-7B-v0.1\", max_length=256, use_4bit=True):\n",
        "        \"\"\"\n",
        "        Initialize the fine-tuner with Mistral model\n",
        "\n",
        "        Args:\n",
        "            model_name: Hugging Face model identifier\n",
        "            max_length: Maximum sequence length for tokenization (reduced for memory)\n",
        "            use_4bit: Use 4-bit quantization to reduce memory usage\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.use_4bit = use_4bit\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.peft_model = None\n",
        "\n",
        "    def setup_model_and_tokenizer(self):\n",
        "        \"\"\"Load and configure the model and tokenizer with memory optimizations\"\"\"\n",
        "        print(f\"Loading {self.model_name} with memory optimizations...\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Configure model loading based on available hardware\n",
        "        model_kwargs = {\n",
        "            \"trust_remote_code\": True,\n",
        "            \"low_cpu_mem_usage\": True,  # Reduce CPU memory usage during loading\n",
        "        }\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            if self.use_4bit:\n",
        "                # 4-bit quantization for maximum memory savings\n",
        "                from transformers import BitsAndBytesConfig\n",
        "                bnb_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.bfloat16\n",
        "                )\n",
        "                model_kwargs.update({\n",
        "                    \"quantization_config\": bnb_config,\n",
        "                    \"device_map\": \"auto\"\n",
        "                })\n",
        "                print(\"Using 4-bit quantization for memory efficiency\")\n",
        "            else:\n",
        "                model_kwargs.update({\n",
        "                    \"torch_dtype\": torch.float16,\n",
        "                    \"device_map\": \"auto\"\n",
        "                })\n",
        "        else:\n",
        "            model_kwargs.update({\n",
        "                \"torch_dtype\": torch.float32,\n",
        "                \"device_map\": None\n",
        "            })\n",
        "\n",
        "        # Load model with memory-efficient settings\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            **model_kwargs\n",
        "        )\n",
        "\n",
        "        # Enable gradient checkpointing to trade compute for memory\n",
        "        self.model.gradient_checkpointing_enable()\n",
        "\n",
        "        # Setup LoRA configuration for efficient fine-tuning\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=8,  # Reduced rank for less memory usage\n",
        "            lora_alpha=16,  # Proportionally reduced\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
        "            bias=\"none\",\n",
        "        )\n",
        "\n",
        "        # Apply LoRA to model\n",
        "        self.peft_model = get_peft_model(self.model, lora_config)\n",
        "        print(\"\\nTrainable parameters:\")\n",
        "        self.peft_model.print_trainable_parameters()\n",
        "\n",
        "        # Clear cache\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def create_sample_dataset(self):\n",
        "        \"\"\"Create a sample dataset for demonstration (sentiment analysis)\"\"\"\n",
        "        data = [\n",
        "            {\"text\": \"I love this product! It's amazing and works perfectly.\", \"label\": \"positive\"},\n",
        "            {\"text\": \"This is the worst purchase I've ever made. Completely disappointed.\", \"label\": \"negative\"},\n",
        "            {\"text\": \"The item is okay, nothing special but does the job.\", \"label\": \"neutral\"},\n",
        "            {\"text\": \"Fantastic quality! Highly recommend to everyone.\", \"label\": \"positive\"},\n",
        "            {\"text\": \"Terrible customer service and poor product quality.\", \"label\": \"negative\"},\n",
        "            {\"text\": \"It's an average product, meets basic expectations.\", \"label\": \"neutral\"},\n",
        "            {\"text\": \"Outstanding! Exceeded all my expectations.\", \"label\": \"positive\"},\n",
        "            {\"text\": \"Waste of money. Doesn't work as advertised.\", \"label\": \"negative\"},\n",
        "            {\"text\": \"Decent product for the price point.\", \"label\": \"neutral\"},\n",
        "            {\"text\": \"Absolutely perfect! Will buy again.\", \"label\": \"positive\"},\n",
        "            {\"text\": \"Poor build quality and arrived damaged.\", \"label\": \"negative\"},\n",
        "            {\"text\": \"It's fine, does what it's supposed to do.\", \"label\": \"neutral\"},\n",
        "            {\"text\": \"Incredible value for money! Love it!\", \"label\": \"positive\"},\n",
        "            {\"text\": \"Horrible experience, would not recommend.\", \"label\": \"negative\"},\n",
        "            {\"text\": \"Standard quality, no complaints but nothing exciting.\", \"label\": \"neutral\"},\n",
        "        ]\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def format_prompt(self, text, label=None):\n",
        "        \"\"\"Format input text into a prompt for the model\"\"\"\n",
        "        if label:\n",
        "            # Training format\n",
        "            return f\"Analyze the sentiment of this text: {text}\\nSentiment: {label}\"\n",
        "        else:\n",
        "            # Inference format\n",
        "            return f\"Analyze the sentiment of this text: {text}\\nSentiment:\"\n",
        "\n",
        "    def prepare_dataset(self, df):\n",
        "        \"\"\"Prepare dataset for training\"\"\"\n",
        "        # Format prompts\n",
        "        formatted_data = []\n",
        "        for _, row in df.iterrows():\n",
        "            prompt = self.format_prompt(row['text'], row['label'])\n",
        "            formatted_data.append({\"text\": prompt})\n",
        "\n",
        "        # Create HuggingFace dataset\n",
        "        dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n",
        "\n",
        "        # Tokenize dataset\n",
        "        def tokenize_function(examples):\n",
        "            tokenized = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "            return tokenized\n",
        "\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=dataset.column_names\n",
        "        )\n",
        "\n",
        "        return tokenized_dataset\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset=None, output_dir=\"./mistral-finetuned\"):\n",
        "        \"\"\"Fine-tune the model with memory-efficient settings\"\"\"\n",
        "        # Check device capabilities\n",
        "        device_has_gpu = torch.cuda.is_available()\n",
        "\n",
        "        # Memory-optimized training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=2,  # Reduced epochs for faster completion\n",
        "            per_device_train_batch_size=1,  # Keep batch size at 1\n",
        "            per_device_eval_batch_size=1,\n",
        "            gradient_accumulation_steps=8,  # Increased to simulate larger batch\n",
        "            warmup_steps=50,  # Reduced warmup steps\n",
        "            learning_rate=2e-4,  # Slightly higher LR for fewer epochs\n",
        "            fp16=device_has_gpu,  # Only use fp16 if GPU is available\n",
        "            logging_steps=5,\n",
        "            save_strategy=\"epoch\",\n",
        "            evaluation_strategy=\"epoch\" if eval_dataset else \"no\",\n",
        "            load_best_model_at_end=False,  # Disable to save memory\n",
        "            report_to=None,  # Disable wandb/tensorboard logging\n",
        "            dataloader_pin_memory=device_has_gpu,  # Only pin memory if GPU available\n",
        "            dataloader_num_workers=0,  # Disable multiprocessing to save memory\n",
        "            remove_unused_columns=False,\n",
        "            gradient_checkpointing=True,  # Enable gradient checkpointing\n",
        "            optim=\"adamw_torch\",  # Use memory-efficient optimizer\n",
        "            max_grad_norm=1.0,  # Gradient clipping\n",
        "            save_total_limit=1,  # Only keep latest checkpoint\n",
        "        )\n",
        "\n",
        "        # Memory-efficient data collator\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,\n",
        "            pad_to_multiple_of=8,  # Efficient padding\n",
        "        )\n",
        "\n",
        "        # Create trainer with memory optimizations\n",
        "        trainer = Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        # Clear cache before training\n",
        "        if device_has_gpu:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Start training with error handling\n",
        "        print(\"Starting memory-efficient training...\")\n",
        "        try:\n",
        "            trainer.train()\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                print(\"\\n❌ OUT OF MEMORY ERROR!\")\n",
        "                print(\"Try these solutions:\")\n",
        "                print(\"1. Reduce max_length further (current: {})\".format(self.max_length))\n",
        "                print(\"2. Use a smaller model (e.g., 'microsoft/DialoGPT-small')\")\n",
        "                print(\"3. Enable 4-bit quantization if not already used\")\n",
        "                print(\"4. Use Google Colab with GPU\")\n",
        "                raise e\n",
        "            else:\n",
        "                raise e\n",
        "\n",
        "        # Save the fine-tuned model\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "        # Clear cache after training\n",
        "        if device_has_gpu:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def inference(self, text, max_new_tokens=50):\n",
        "        \"\"\"Run inference on the fine-tuned model\"\"\"\n",
        "        prompt = self.format_prompt(text)\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract just the generated part\n",
        "        generated_text = response[len(prompt):].strip()\n",
        "        return generated_text\n",
        "\n",
        "def main():\n",
        "    # Initialize fine-tuner\n",
        "    fine_tuner = MistralFineTuner()\n",
        "\n",
        "    # Setup model and tokenizer\n",
        "    fine_tuner.setup_model_and_tokenizer()\n",
        "\n",
        "    # Create or load your dataset\n",
        "    df = fine_tuner.create_sample_dataset()\n",
        "    print(f\"Dataset size: {len(df)}\")\n",
        "    print(\"\\nSample data:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Split data for training and validation\n",
        "    train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Prepare datasets\n",
        "    train_dataset = fine_tuner.prepare_dataset(train_df)\n",
        "    eval_dataset = fine_tuner.prepare_dataset(eval_df)\n",
        "\n",
        "    # Fine-tune the model\n",
        "    fine_tuner.train(train_dataset, eval_dataset)\n",
        "\n",
        "    # Test inference\n",
        "    test_texts = [\n",
        "        \"This product is absolutely wonderful!\",\n",
        "        \"I hate this thing, it's broken.\",\n",
        "        \"It's an okay product, nothing special.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TESTING FINE-TUNED MODEL\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for text in test_texts:\n",
        "        result = fine_tuner.inference(text)\n",
        "        print(f\"\\nInput: {text}\")\n",
        "        print(f\"Predicted sentiment: {result}\")\n",
        "\n",
        "# Additional utility functions for custom datasets\n",
        "\n",
        "def load_custom_dataset(file_path, text_column, label_column):\n",
        "    \"\"\"Load custom dataset from CSV file\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df[[text_column, label_column]].rename(\n",
        "        columns={text_column: 'text', label_column: 'label'}\n",
        "    )\n",
        "\n",
        "def prepare_instruction_dataset(data, instruction_template=None):\n",
        "    \"\"\"Prepare dataset in instruction-following format\"\"\"\n",
        "    if instruction_template is None:\n",
        "        instruction_template = \"Complete the following task: {instruction}\\n\\nInput: {input}\\n\\nOutput: {output}\"\n",
        "\n",
        "    formatted_data = []\n",
        "    for item in data:\n",
        "        formatted_text = instruction_template.format(\n",
        "            instruction=item.get('instruction', ''),\n",
        "            input=item.get('input', ''),\n",
        "            output=item.get('output', '')\n",
        "        )\n",
        "        formatted_data.append({\"text\": formatted_text})\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if GPU is available and configure accordingly\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU available: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "        print(\"Using GPU acceleration with optimized settings\")\n",
        "    else:\n",
        "        print(\"No GPU available, using CPU\")\n",
        "        print(\"Warning: Training will be significantly slower on CPU\")\n",
        "        print(\"Consider using Google Colab or other GPU platforms for faster training\")\n",
        "\n",
        "    # Additional memory optimization for CPU-only setups\n",
        "    if not torch.cuda.is_available():\n",
        "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "        torch.set_num_threads(min(4, torch.get_num_threads()))  # Limit CPU threads\n",
        "\n",
        "    # Run the main fine-tuning process\n",
        "    main()"
      ]
    }
  ]
}