{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwsoV2/IpurmDcPRn+bllI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishnuy/AI-ML/blob/main/TrainMistralSentiAnalysis2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import os\n",
        "\n",
        "class MistralFineTuner:\n",
        "    def __init__(self, model_name=\"mistralai/Mistral-7B-v0.1\", max_length=512):\n",
        "        \"\"\"\n",
        "        Initialize the fine-tuner with Mistral model\n",
        "\n",
        "        Args:\n",
        "            model_name: Huging Face model identifier\n",
        "            max_length: Maximum sequence length for tokenization\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.peft_model = None\n",
        "\n",
        "    def setup_model_and_tokenizer(self):\n",
        "        \"\"\"Load and configure the model and tokenizer\"\"\"\n",
        "        print(f\"Loading {self.model_name}...\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Load model with appropriate settings for fine-tuning\n",
        "        # Load with device_map=\"auto\" initially\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Explicitly move the base model to the target device if using device_map=\"auto\"\n",
        "        # This might help resolve meta tensor issues before applying PEFT\n",
        "        # target_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        # self.model.to(target_device)\n",
        "\n",
        "\n",
        "        # Setup LoRA configuration for efficient fine-tuning\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=16,  # Rank\n",
        "            lora_alpha=32,  # LoRA scaling parameter\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "        )\n",
        "\n",
        "        # Apply LoRA to model\n",
        "        self.peft_model = get_peft_model(self.model, lora_config)\n",
        "        self.peft_model.print_trainable_parameters()\n",
        "\n",
        "    def create_sample_dataset(self):\n",
        "        \"\"\"Create a sample dataset for demonstration (sentiment analysis)\"\"\"\n",
        "        data = [\n",
        "            {\"text\": \"I love this product! It's amazing and works perfectly.\", \"label\": \"positive\"},\n",
        "            {\"text\": \"This is the worst purchase I've ever made. Completely disappointed.\", \"label\": \"negative\"},\n",
        "            {\"text\": \"The item is okay, nothing special but does the job.\", \"label\": \"neutral\"},\n",
        "            {\"text\": \"Fantastic quality! Highly recommend to everyone.\", \"label\": \"positive\"},\n",
        "            {\"text\": \"Terrible customer service and poor product quality.\", \"label\": \"negative\"},\n",
        "            {\"text\": \"It's an average product, meets basic expectations.\", \"label\": \"neutral\"},\n",
        "            {\"text\": \"Outstanding! Exceeded all my expectations.\", \"label\": \"positive\"},\n",
        "            {\"text\": \"Waste of money. Doesn't work as advertised.\", \"label\": \"negative\"},\n",
        "            {\"text\": \"Decent product for the price point.\", \"label\": \"neutral\"},\n",
        "            {\"text\": \"Absolutely perfect! Will buy again.\", \"label\": \"positive\"},\n",
        "            {\"text\": \"Poor build quality and arrived damaged.\", \"label\": \"negative\"},\n",
        "            {\"text\": \"It's fine, does what it's supposed to do.\", \"label\": \"neutral\"},\n",
        "            {\"text\": \"Incredible value for money! Love it!\", \"label\": \"positive\"},\n",
        "            {\"text\": \"Horrible experience, would not recommend.\", \"label\": \"negative\"},\n",
        "            {\"text\": \"Standard quality, no complaints but nothing exciting.\", \"label\": \"neutral\"},\n",
        "        ]\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def format_prompt(self, text, label=None):\n",
        "        \"\"\"Format input text into a prompt for the model\"\"\"\n",
        "        if label:\n",
        "            # Training format\n",
        "            return f\"Analyze the sentiment of this text: {text}\\nSentiment: {label}\"\n",
        "        else:\n",
        "            # Inference format\n",
        "            return f\"Analyze the sentiment of this text: {text}\\nSentiment:\"\n",
        "\n",
        "    def prepare_dataset(self, df):\n",
        "        \"\"\"Prepare dataset for training\"\"\"\n",
        "        # Format prompts\n",
        "        formatted_data = []\n",
        "        for _, row in df.iterrows():\n",
        "            prompt = self.format_prompt(row['text'], row['label'])\n",
        "            formatted_data.append({\"text\": prompt})\n",
        "\n",
        "        # Create HuggingFace dataset\n",
        "        dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n",
        "\n",
        "        # Tokenize dataset\n",
        "        def tokenize_function(examples):\n",
        "            tokenized = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            tokenized[\"labels\"] = tokenized[\"input_ids\"].clone() # Use .clone() for tensors\n",
        "            return tokenized\n",
        "\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=dataset.column_names\n",
        "        )\n",
        "\n",
        "        return tokenized_dataset\n",
        "\n",
        "    def train(self, train_dataset, eval_dataset=None, output_dir=\"./mistral-finetuned\"):\n",
        "        \"\"\"Fine-tune the model\"\"\"\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=1,  # Small batch size for limited resources\n",
        "            per_device_eval_batch_size=1,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=100,\n",
        "            learning_rate=5e-5,\n",
        "            fp16=True,  # Use mixed precision for efficiency\n",
        "            logging_steps=10,\n",
        "            save_strategy=\"epoch\",\n",
        "            eval_strategy=\"epoch\" if eval_dataset else \"no\", # Changed evaluation_strategy to eval_strategy\n",
        "            load_best_model_at_end=True if eval_dataset else False,\n",
        "            metric_for_best_model=\"eval_loss\" if eval_dataset else None,\n",
        "            report_to=None,  # Disable wandb/tensorboard logging\n",
        "            gradient_checkpointing=True, # Enable gradient checkpointing\n",
        "        )\n",
        "\n",
        "        # Data collator\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,  # Causal LM, not masked LM\n",
        "        )\n",
        "\n",
        "        # The model should already be on the correct device after setup_model_and_tokenizer\n",
        "        # but explicitly move it again just in case, though this might be redundant now.\n",
        "        # self.peft_model = self.peft_model.to(training_args.device)\n",
        "\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = Trainer(\n",
        "            model=self.peft_model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        # Start training\n",
        "        print(\"Starting training...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Save the fine-tuned model\n",
        "        trainer.save_model()\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "        print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "    def inference(self, text, max_new_tokens=50):\n",
        "        \"\"\"Run inference on the fine-tuned model\"\"\"\n",
        "        prompt = self.format_prompt(text)\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract just the generated part\n",
        "        generated_text = response[len(prompt):].strip()\n",
        "        return generated_text"
      ],
      "metadata": {
        "id": "B8nz4HV7F3xH"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}